---
title: "Client Report - Machine Learning"
subtitle: "Course DS 250"
author: "Logan Clark"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---


```{python}
#| label: libraries
#| include: false
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import f1_score, matthews_corrcoef
```


## Elevator pitch

I discovered that features such as living area, basement, presence, and construction year are crucial in this prediciton. Using a Descision Tree Classifier, I achieved over 90% accuracy in labeling houses. In my report it will show a heat map of how to determine if it was before, during, or after 1980.

```{python}
#| label: project data
#| code-summary: Read and format project data
# Include and execute your code here
dwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')
```

## Relationship Between Home Variables and before1980

Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.

From these charts, we can conclude that certain home variables such as the living area, the presence of a basement, and the year the house was built are important factors in determining whether a house was built before or after 1980. In the first chart we can see the heat map of living area, finished basement, and basement compared to the year. The yellow shows the homes before 1980 and the blue shows homes after 1980. The homes that had finished basements were most likley built before 1980. The ROC curve helps us to see how well our guessing is. So in this case we can assume and make a well educated guess if the home was built before or after 1980. These insights can help a machine learning algorithm make more accurate predictions by focusing on these influential features during training. Additionally, understanding these relationships can provide valuable insights for real estate professionals and policymakers.

```{python}
#| label: Q1
#| code-summary: Read and format data
# Include and execute your code here
# Feature selection: select features excluding 'before1980', 'yrbuilt', and 'parcel' columns
X_pred = dwellings_ml.drop(dwellings_ml.filter(regex='before1980|yrbuilt|parcel').columns, axis=1)

# Target variable
y_pred = dwellings_ml.filter(regex="before1980")

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.34, random_state=76)

# Initialize and train DecisionTreeClassifier
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)

# Predicted probabilities
y_probs = clf.predict_proba(X_test)

h_subset = dwellings_ml.filter(
    ['livearea', 'finbsmnt', 'basement', 
    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',
    'stories', 'yrbuilt']).sample(500)
chart = px.scatter_matrix(h_subset,
    dimensions=['livearea', 'finbsmnt', 'basement'],
    color='before1980'
)
chart.update_traces(diagonal_visible=False)
chart.show()

# Predicted probabilities
y_probs = clf.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

# Compute AUC score
auc_score = roc_auc_score(y_test, y_probs)

# Create a DataFrame for ROC curve
roc_df = pd.DataFrame({'False Positive Rate': fpr, 'True Positive Rate': tpr})

# Plot ROC curve using Plotly Express
fig = px.line(roc_df, x='False Positive Rate', y='True Positive Rate', title=f'ROC Curve (AUC={auc_score:.2f})')
fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)
fig.show()
```


## Build a Classification Model

Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.

The classification model for labeling houses as built "before 1980" or "during or after 1980" achieved over 90% accuracy using a Decision Tree Classifier. Features excluding 'before1980', 'yrbuilt', and 'parcel' columns were selected for training the model, which was then evaluated using metrics such as accuracy, precision, recall, and F1-score. Logistic regression and random forest classifiers were also tested, but the decision tree classifier was chosen for its simplicity, interpretability, and high accuracy.

```{python}
#| label: Q2
#| code-summary: Read and format data
# Include and execute your code here
print(metrics.classification_report(y_pred, y_test))

```

## Justify Choice

Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.

Based on the chart, it is evident that the most important features are 'livearea', 'yearbuilt', and 'basement'. 'Livearea' refers to the total living area of the house, which could be indicative of the size and layout of the property. 'Yearbuilt' directly indicates when the house was constructed and is crucial for determining its age. Lastly, 'basement' signifies whether the property has a basement or not, which could influence its construction style and value. These features are likely influential in predicting the construction era of the houses with high accuracy.

```{python}
#| label: Q3
#| code-summary: Read and format data
# Include and execute your code here
df_features = pd.DataFrame(
    {'f_names': X_train.columns, 
    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)

chart = px.bar(df_features.head(10),
    x='f_values', 
    y='f_names'
)

chart.update_layout(yaxis={'categoryorder':'total ascending'})

```

## Quality of Classification Model

Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.

Accuracy: Accuracy measures the proportion of correctly classified instances out of the total instances. It is calculated as the ratio of the number of correct predictions to the total number of predictions. A higher accuracy indicates better performance, but it may not be the most reliable metric if the classes are imbalanced.

Precision: Precision measures the proportion of true positive predictions out of all positive predictions (both true positives and false positives). It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision is useful when the cost of false positives is high. Higher precision indicates fewer false positives.

Recall: Recall measures the proportion of true positive predictions out of all actual positive instances. It is calculated as the ratio of true positives to the sum of true positives and false negatives. Recall is useful when the cost of false negatives is high. Higher recall indicates fewer false negatives.

High accuracy indicates that the model is making correct predictions overall.

High precision indicates that when the model predicts a positive outcome, it is likely to be correct.

High recall indicates that the model is capturing a large proportion of positive instances.

```{python}
#| label: Q4
#| code-summary: Read and format data
# Include and execute your code here
accuracy = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred)
recall = metrics.recall_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

# f1 = f1_score(y_test, y_pred)
# roc_auc = roc_auc_score(y_test, y_pred)
# mcc = matthews_corrcoef(y_test, y_pred)

# print(f"F1 Score: {f1:.2f}")
# print(f"ROC AUC Score: {roc_auc:.2f}")
# print(f"Matthews Correlation Coefficient: {mcc:.2f}")
```